<!DOCTYPE html>
<html>
<body>
  <h1>Methods:</h1>
  <h3>Data scraping and API Methods</h3>
  <p> We first accessed the <a href="https://openlibrary.org/developers/dumps">Open Library Data Dumps</a>, which provides metadata records and Internet Archive identifiers for a substantial subset (roughly 4 million books out of 8 million total) of the overall IA collection. </p>
  <p> We then used the IA identifiers to programatically query the Internet Archive API using the <a href="https://archive.org/developers/quick-start-pip.html">IA python package</a>, keeping the metadata fields relevant for the creative process.</p>
  
  <p>Metadata key</a>
  <ul>
    <li>ID: Internet Archive unique identifier</li>
    <li>Title</li>
    <li>Author</li>
     <li>Camera: tech spec of camera used to scan</li>
     <li>Contributor: Organization owner who provided book</li>
     <li>publish_date: date original work published</li>
     <li>language: language of original book</li>
     <li>operator: name of worker performing scan</li>
     <li>ppi: resolution of scan</li>
     <li>repub_state: unknown</li>
     <li>scanner: machine used to perform scan</li>
     <li>scanningcenter: location scan performed</li>
     <li>sponsor: entity paying for scan</li>
     <li>scandate: date scanned (deprecated, use date)</li>
     <li>imagecount: number of pages</li>
     <li>republisher_operator: worker performing republishing (editing, quality control, formatting)</li>
     <li>republisher_date: day republishing labor performed</li>
    <li>republisher_time: time republishing labor performed</li>
    <li>scanfee: amount charged for scanning</li>
    <li>sponsordate: unknown</li>
    <li>ocr_converted: software used to perform OCR on scan</li>
    <li>page_number_confidence: unknown</li>
    <li>search_date: date added to IA records</li>
    <li>date: cleaned and formatted date</li>
  
  </ul>
  <p>Complete and processed data files are available at <a href="https://uofi.box.com/s/2hvihu1tp4t2m0sapdb7wbjkl2zvpudf">this Box site</a>. </p>
    <ul>
      <li>IA_metadata_full.csv: complete scraped metadata for 3 million books</li>
      <li>centers_per_year.csv: books scanned at each center per month</li>
      <li>per_center_stats.csv: finalized list of geocoded centers with various aggregate statistics</li>
  </ul>
  
  <p>Jupyter notebooks used to scrape, clean, and create visualizations along with our website code are available at <a href="https://github.com/scanninglabor/IAScanningLabor">our Github repo</a></p>
  
  <h3>Mapping Methods</h3>
  <h4>Geocoding the centers</h4>
<p>Geocoding refers to the process through which geographers (or GIS systems) transform language referring to a place into mappable geographic coordinates. GIS systems can often automatically geocode information like addresses, city names, or country names. However, they are incapable of resolving less standardized place references to geographic coordinates. In that case, a human must geocode them.</p>
<p>For the Scanning Labor project, we geocoded the metadata field, “scanningcenter” in the downloaded Open LIbrary records. Of the 3 million records we scraped from the Open Library API, only 2.5 million had any information in the “scanningcenter” field. Workers created these 500,000 records mostly from 2001 to 2008 under the purview of the Google Books project. Google, unlike IA, required employees to sign NDAs to not reveal the scanning center location.  As such, we cannot geocode the centers at which workers scanned these 500,000 books based on the records we have. This geographic opacity is strategic on Google’s part.</p>
<p>The other 2.5 million records contain 93 unique values in the “scanningcenter” field. We attempted to geocode these through the following method:</p>
<ol type=”1”>
<li>For each value, we entered the following query in the archive.org search bar: “scanningcenter:(value)”. For example, to geocode the value “il”, we searched “scanningcenter:(il)” </li>
<li>Next, we panned over the books on the result list. If they all appeared to be part of the same collection, we navigated to the collection page. If they were not part of the same collection or series of collections, we skipped down to step 4.</li>
<li>If the number of books in the collection was similar to the number of books the search returned, it is likely that the institutional owner of the collection is the same as that which housed the scanning center. According to the director of IA’s scanning partners program, Elizabeth McCleod, most of the scanning centers are located within the partner institution’s library. Therefore, we geocoded these centers to the partner institution’s library’s geographic coordinates.</li>
<li>Scanning centers for which the query did not return books belonging to a single collection were probably operated by internet archive to digitize its own collections. We browsed the books in this result to confirm this. If the “sponsor” field or “owner” field contained “Internet Archive” or “Kahle/Austin” foundation, we deduced these likely IA-owned books. Geocoding these centers was more complicated, as most are not associated with any partner institution and many are contracted business processing outsourcing (BPOs) organizations in East or Southeast Asia.</li>
<li>To geocode them, we searched for the “scanningcenter” value--i.e., cebu--in the Internet Archive blog. Sometimes, this returned blog entries announcing a new relationship with a BPO company. Other times, there were no blog entries, and so we searched the Internet Archive’s 990 tax returns for contractor partners that may correspond to “scanningcenter” values. For example, the value “shenzhen” goes unmentioned in any IA blog posts. However, IA’s 990s reveal the organization began contracting with a Chinese BPO firm, Datum Data Co. Ltd., located in Shenzhen at the same time the “scanningcenter” value “shenzhen” appeared in the data.</li>
<li>We were unable to geocode a few of the “scanningcenter” values. We decided to map these in spite of our inability to accurately geocode them because we did not want to further invisibilize the labor of the workers who scanned the books and created the records. However, we distinguished these from centers with locations we successfully geocoded using an aqua dot instead of red.</li> 
<li>The 93 unique values in the “scanningcenter” field corresponded to 64 scanning center locations. For example, “il”, “ill”, and “illinois” all refer to the University of Illinois at Urbana--Champaign. We made the list of tags, coordinates, and names of the scanning center available in the scanning center location map. We separated the “scanningcenter” tags for each center with “||”</li>
</ol>
<h4>Data Preparation for mapping</h4>
<ol type=”1”>
<li>We created a python script to process the downloaded data and count the number of records per “scanningcenter” tag per month. The result is a spreadsheet where each row represents a year in a month and each column represents the number of records tagged with the “scanningcenter” value for that month. (insert link to Lucian’s script here)</li>
<li>Next, we geocoded the data. To do this, we iterated over every record in the scans/tag/month dictionary and mapped each tag to the center it corresponded with, adding together the number of records along the way. For example, if there were 90 records tagged with “il” and 3 tagged with “ill” in January 2018, there should be 93 scans at the University of Illinois during that time period. If the number of records digitized in a month was 0, we did not include the center on the map (instead, assuming it closed).</li>
<li>We transformed each date into YYYY-MM-DD HH:MM:SS format. </li>
<li>We created a csv file where each row corresponds to a scanning center for each month of the period of time between December 2001 to October 2022. See creates_csv_map.py</li>
</ol>
<h4>Making the KeplerGL map</h4>
<p>KeplerGL is an open source mapping platform run on Uber’s mapping API. We decided to use it in lieu of Esri's ArcGIS because it is open source and allows for easy export to HTML. We uploaded the csv file containing the number of scans per center per month to Kepler’s online user interface. From there, we made the radius of each point correspond to the number of scans, “count.” Next, we made the color of the point dependent on how certain we were that we had geocoded the location properly. Finally, we added a time filter to create the animation.</p>
<p>The python script with which we created the csv file, the csv file, along with the map as a json file are all available on our GitHub.</p>


  <h3>Oral History Methods</h3>
  <p>Initially, our goal for this project was to include oral histories of scan operators at each of the scanning centers. To do so, we developed a series of questions found here: <a href="https://docs.google.com/document/d/18msJjzixYps1LA95IArxlGpROqFgDQLf88edpxUHlvQ/edit?usp=sharing">DH-IA-oral history questions</a> and planned to conduct one-hour oral interviews to capture the day-to-day experience of scanning for IA. </p>
  <p> We scraped over 500 emails from the metadata and reached out individually to 25 of those people. Unfortunately, we had trouble connecting with any of them - almost all of the emails bounced back. This highlights the high turnover rate of scanners at IA that contributes to the invisibility of these workers. We did receive one reply from a scan operator: “Thank you for considering me for this project. Unfortunately I don't feel I would be a good fit at this time. Please contact Chris Freeland at chrisfreeland@archive.org. Chris is our PR representative. I'm sure they will be able to help you.” We reached out to Chris Freeland at Internet Archive but did not receive a reply. </p>
  <p> Pivoting our approach, we created a google survey to send out oral history questions via email. We felt a google survey would lead to more responses as it takes less time from interviewees and is completely anonymous. We wanted to be cognizant of the extra time and unpaid labor we would be asking of these workers, so the survey is only 13 questions and none of them are required. We were able to send this survey to all 500 of the email addresses that were in the metadata, so we had a broader reach and higher likelihood of connecting with workers who were interested and emails were still active. We created a <a href="https://docs.google.com/forms/d/e/1FAIpQLScYci2RBW5-j2baAs8BRS7Xdxgz3PKrzzo6UygPXrIRgduHqQ/viewform?usp=sf_link">Google forms survey</a>. So far we’ve only received three responses and many bounced back emails. </p>
  <p> Beyond gathering stories and anecdotes through oral history and survey, we also sought out pre-existing worker narratives on glassdoor. From our experience, it seems crucial to approach oral history more intentionally and slowly, to be realisitic about the commitment to relationship building it entials. </p>
  <p> Another aspect of oral history in our project was to meet with several non-scanning staff at the Internet Archive (although two of the people we interviewed had started as scanners). We conducted two hour long interviews to understand the workflow of scanning centers, better understand our findings in the metadata, and get contacts for further interviews. </p>
  <p> While IA middle management were initially willing to meet with us and interview them for the project, they refused to go forward with the project after we sent out our survey to IA scanning center workers. After sending the survey, we received this email from an IA staff member: “I'm glad our conversation was helpful for your project. At this point, we have participated as an organization to the extent that we are comfortable. If you have any further inquiries, please direct them to me.” </p>
  <p> </p>
  </body>
</html>
